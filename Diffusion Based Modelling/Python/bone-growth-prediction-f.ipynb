{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":15122,"sourceType":"datasetVersion","datasetId":10832}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Installations and Imports**","metadata":{}},{"cell_type":"code","source":"# Install required packages quietly (suppress output)\n!pip -q install imageio scikit-image pytorch-msssim lpips diffusers matplotlib --no-input >/dev/null\n\n# Import necessary libraries\nimport os, math, random, glob, json, re\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageDraw, ImageFont\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split, Subset\nfrom torchvision import transforms\nimport cv2\nfrom skimage.metrics import structural_similarity as skimage_ssim\nfrom pytorch_msssim import ms_ssim as torch_ms_ssim\nimport lpips\nfrom diffusers import UNet2DModel\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:17:41.107847Z","iopub.execute_input":"2025-10-02T16:17:41.108151Z","iopub.status.idle":"2025-10-02T16:19:22.728028Z","shell.execute_reply.started":"2025-10-02T16:17:41.108129Z","shell.execute_reply":"2025-10-02T16:19:22.727366Z"}},"outputs":[{"name":"stderr","text":"2025-10-02 16:19:10.311346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759421950.510253      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759421950.568648      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Configuration Settings**","metadata":{}},{"cell_type":"code","source":"# Configuration class to store all hyperparameters and settings\nclass CFG:\n    SEED = 42  # Random seed for reproducibility\n    IMG_SIZE = 256  # Size of input images (256x256)\n    BATCH = 8  # Batch size for training\n    EPOCHS = 10  # Number of training epochs (reduced for GPU limits)\n    LR = 1e-4  # Learning rate\n    MAX_SAMPLES = None  # Maximum number of samples to use (None for full dataset)\n    NUM_WORKERS = 2  # Number of workers for data loading\n    TARGET_AGE = 216  # Target age in months (18 years)\n    T_STEPS = 1000  # Number of timesteps for diffusion\n    DDIM_STEPS = 30  # Number of steps for DDIM sampling\n    CFG_DROP = 0.1  # Dropout rate for classifier-free guidance\n    CFG_WEIGHT = 2.0  # Weight for classifier-free guidance\n    AMP = True  # Use automatic mixed precision\n    ACCUM_STEPS = 2  # Gradient accumulation steps\n    CLIP_NORM = 1.0  # Gradient clipping norm\n    SAVE_DIR = \"/kaggle/working\"  # Directory to save models\n    OUT_DIR = \"/kaggle/working/out_216\"  # Directory to save outputs\n    COMPILE = False  # Disable model compilation (avoid Dynamo issues)\n    DATASET_HINTS = [  # Possible paths for the dataset\n        \"/kaggle/input/rsna-bone-age\",\n        \"/kaggle/input/rsna-bone-age/rsna-bone-age\",\n        \"/kaggle/input\"\n    ]\n    MEAS_XLSX = \"/kaggle/working/measurements_export.xlsx\"  # Path to save Excel measurements\n    AREA_LAMBDA = 3.0  # Weight for area loss term\n    SOFT_K = 7.0  # Softmax scaling factor for area calculation\n    AREA_MIN = 0.02  # Minimum area fraction\n    AREA_MAX = 0.75  # Maximum area fraction\n    ATTN_HEADS = 4  # Number of attention heads\n    ATTN_DIM = 32  # Dimension per attention head\n\n# Set random seed for reproducibility\ndef seed_everything(seed: int = 42):\n    random.seed(seed)  # Set Python random seed\n    np.random.seed(seed)  # Set NumPy random seed\n    torch.manual_seed(seed)  # Set PyTorch random seed\n    torch.cuda.manual_seed_all(seed)  # Set PyTorch CUDA random seed\n    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n    torch.backends.cudnn.benchmark = False  # Disable benchmarking for reproducibility\n\n# Apply seeding\nseed_everything(CFG.SEED)\n\n# Set device (GPU if available, else CPU)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:22.729227Z","iopub.execute_input":"2025-10-02T16:19:22.729741Z","iopub.status.idle":"2025-10-02T16:19:22.740434Z","shell.execute_reply.started":"2025-10-02T16:19:22.729717Z","shell.execute_reply":"2025-10-02T16:19:22.739727Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**Data Discovery Functions**","metadata":{}},{"cell_type":"code","source":"# Function to find the root directory of the RSNA Bone Age dataset\ndef find_rsna_boneage_root() -> Tuple[str, str]:\n    candidates = []  # List to store potential dataset locations\n    \n    # Search through possible dataset paths\n    for base in CFG.DATASET_HINTS:\n        if not os.path.exists(base):\n            continue  # Skip if path doesn't exist\n            \n        # Look for the CSV file\n        for csv_path in glob.glob(os.path.join(base, \"**\", \"boneage-training-dataset.csv\"), recursive=True):\n            root = os.path.dirname(csv_path)  # Get directory containing CSV\n            preferred = os.path.join(root, \"boneage-training-dataset\")  # Preferred image directory\n            \n            # Create list of potential image directories\n            dirs = [preferred] if os.path.isdir(preferred) else []\n            dirs += [d for d in glob.glob(os.path.join(root, \"**\"), recursive=True) if os.path.isdir(d)]\n            \n            # Find directory with most images\n            best_dir, best_cnt = \"\", -1\n            for d in dirs:\n                cnt = (len(glob.glob(os.path.join(d, \"*.png\"))) +  # Count PNG files\n                       len(glob.glob(os.path.join(d, \"*.jpg\"))) +  # Count JPG files\n                       len(glob.glob(os.path.join(d, \"*.jpeg\"))))  # Count JPEG files\n                if cnt > best_cnt:\n                    best_dir, best_cnt = d, cnt\n            \n            candidates.append((csv_path, best_dir, best_cnt))\n    \n    # Handle case where no dataset is found\n    if not candidates:\n        raise FileNotFoundError(\"Attach Kaggle dataset 'kmader/rsna-bone-age' via Add data.\")\n    \n    # Select the candidate with the most images\n    csv_path, img_dir, cnt = sorted(candidates, key=lambda x: -x[2])[0]\n    print(f\"[data] CSV: {csv_path}\")\n    print(f\"[data] IMG_DIR: {img_dir} (files: {cnt})\")\n    return csv_path, img_dir\n\n# Regular expression to extract ID from image filenames\n_ID_RE = re.compile(r\"(\\d+)\\.(png|jpg|jpeg)$\", re.IGNORECASE)\n\n# Function to index images in the directory\ndef index_image_dir(img_dir: str) -> Dict[int, str]:\n    idx: Dict[int, str] = {}  # Dictionary to map image IDs to paths\n    \n    # Recursively search for image files\n    for path in glob.glob(os.path.join(img_dir, \"**\", \"*.*\"), recursive=True):\n        if not os.path.isfile(path): \n            continue  # Skip if not a file\n            \n        # Extract ID from filename\n        m = _ID_RE.search(os.path.basename(path))\n        if not m: \n            continue  # Skip if no match\n            \n        try:\n            img_id = int(m.group(1))  # Convert ID to integer\n            # Update path if this is the shortest path (most direct)\n            if img_id not in idx or path.count(os.sep) < idx[img_id].count(os.sep):\n                idx[img_id] = path\n        except Exception:\n            continue  # Skip if ID conversion fails\n    \n    print(f\"[data] Indexed {len(idx)} images.\")\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:22.741195Z","iopub.execute_input":"2025-10-02T16:19:22.741439Z","iopub.status.idle":"2025-10-02T16:19:24.312598Z","shell.execute_reply.started":"2025-10-02T16:19:22.741417Z","shell.execute_reply":"2025-10-02T16:19:24.311878Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"**Image Processing Utilities**","metadata":{}},{"cell_type":"code","source":"# Function to load grayscale PNG/JPG images and resize if needed\ndef load_gray_png_any(path: str, size: int = CFG.IMG_SIZE) -> Image.Image:\n    im = Image.open(path).convert(\"L\")  # Open image and convert to grayscale\n    if im.size != (size, size):\n        im = im.resize((size, size), Image.LANCZOS)  # Resize if not target size\n    return im\n\n# Function to convert float array [0,1] to uint8 [0,255]\ndef to_uint8(x: np.ndarray) -> np.ndarray:\n    x = np.clip(x, 0.0, 1.0)  # Clip values to [0,1]\n    return (x * 255.0 + 0.5).astype(np.uint8)  # Scale and convert to uint8\n\n# Function to convert uint8 [0,255] to float [0,1]\ndef from_uint8(x: np.ndarray) -> np.ndarray:\n    return x.astype(np.float32) / 255.0  # Scale to [0,1]\n\n# Function to convert PIL image to PyTorch tensor (normalized to [-1,1])\ndef pil_to_tensor_gray(im: Image.Image) -> torch.Tensor:\n    arr = np.array(im, dtype=np.float32) / 255.0  # Convert to numpy and normalize to [0,1]\n    arr = (arr - 0.5) / 0.5  # Normalize to [-1,1]\n    return torch.from_numpy(arr)[None, None, :, :].contiguous()  # Convert to tensor and add batch/channel dims\n\n# Function to convert PyTorch tensor to PIL image\ndef tensor_to_pil_gray(t: torch.Tensor) -> Image.Image:\n    t = t[0, 0].detach().cpu().clamp(-1, 1).numpy()  # Remove batch/channel, clamp to [-1,1], convert to numpy\n    t = (t + 1) * 0.5  # Normalize to [0,1]\n    return Image.fromarray(to_uint8(t))  # Convert to PIL image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.314074Z","iopub.execute_input":"2025-10-02T16:19:24.314307Z","iopub.status.idle":"2025-10-02T16:19:24.327471Z","shell.execute_reply.started":"2025-10-02T16:19:24.314289Z","shell.execute_reply":"2025-10-02T16:19:24.326909Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**Robust Measurements and Mask Generation**","metadata":{}},{"cell_type":"code","source":"# Function to find the largest connected component in a binary image\ndef _largest_cc(bin_img: np.ndarray) -> np.ndarray:\n    n, labels = cv2.connectedComponents(bin_img.astype(np.uint8))  # Get connected components\n    if n <= 1:\n        return bin_img  # Return original if only one component (background)\n    \n    # Find area of each component\n    areas = [(labels == i).sum() for i in range(1, n)]\n    idx = int(np.argmax(areas)) + 1  # Index of largest component\n    return (labels == idx).astype(np.uint8)  # Return mask of largest component\n\n# Function to calculate major span using PCA\ndef _pca_major_span(coords_full: np.ndarray) -> float:\n    if coords_full.shape[0] < 5:\n        return 0.0  # Return 0 if too few points\n    \n    x = coords_full[:, [1,0]].astype(np.float32)  # Swap x,y for PCA\n    x -= x.mean(0, keepdims=True)  # Center the data\n    cov = (x.T @ x) / max(1, len(x)-1)  # Compute covariance matrix\n    vals, vecs = np.linalg.eigh(cov)  # Eigen decomposition\n    v = vecs[:, np.argmax(vals)]  # Principal component\n    proj = x @ v  # Project data onto principal component\n    return float(proj.max() - proj.min())  # Return span\n\n# Function to compute robust measurements and generate mask\ndef robust_measurements_and_mask(img_u8: np.ndarray) -> Tuple[dict, np.ndarray]:\n    try:\n        h, w = img_u8.shape  # Get image dimensions\n        b = max(1, int(0.03 * min(h, w)))  # Border size (3% of min dimension)\n        roi = img_u8[b:h-b, b:w-b]  # Define region of interest (excluding border)\n        blur = cv2.GaussianBlur(roi, (5,5), 0)  # Apply Gaussian blur\n        thr, _ = cv2.threshold(blur, 0, 255, cv2.THRESH_OTSU)  # Otsu thresholding\n        m = (blur > thr).astype(np.uint8)  # Create binary mask\n        \n        # Define morphological kernels\n        k3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n        k5 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n        \n        # Apply morphological operations\n        m = cv2.morphologyEx(m, cv2.MORPH_OPEN, k3)  # Opening to remove small noise\n        m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, k5)  # Closing to fill small holes\n        m = _largest_cc(m)  # Keep only largest connected component\n        \n        # Create full-size mask\n        mask_full = np.zeros_like(img_u8, dtype=np.uint8)\n        mask_full[b:h-b, b:w-b] = m\n        \n        # Get coordinates of mask pixels\n        ys, xs = np.where(mask_full > 0)\n        area = float(mask_full.sum())  # Calculate area\n        \n        # Handle case where mask is empty\n        if ys.size == 0:\n            dens = float(img_u8.mean())\n            return dict(area=0.0, height=0.0, width=0.0, aspect=0.0, density=dens, width_length_ratio=0.0), mask_full\n        \n        # Calculate bounding box\n        ymin, ymax = ys.min(), ys.max()\n        xmin, xmax = xs.min(), xs.max()\n        bbox_h = float((ymax - ymin + 1))\n        bbox_w = float((xmax - xmin + 1))\n        span = _pca_major_span(np.stack([ys, xs], 1))  # Calculate PCA span\n        \n        # Calculate height and width\n        height = max(bbox_h, span)\n        width = bbox_w\n        aspect = height / max(1.0, width)\n        dens = float(img_u8[mask_full > 0].mean())  # Mean density in mask\n        frac = area / float(h * w)  # Fraction of image covered by mask\n        \n        # Handle case where mask covers most of image\n        if frac > 0.85 and height >= (h * 0.98):\n            p = np.percentile(roi, 85)  # Get 85th percentile\n            m2 = (roi > p).astype(np.uint8)  # Create new mask with higher threshold\n            m2 = cv2.morphologyEx(m2, cv2.MORPH_OPEN, k3)  # Apply opening\n            m2 = _largest_cc(m2)  # Keep largest component\n            \n            # Update full-size mask\n            mask_full = np.zeros_like(img_u8, dtype=np.uint8)\n            mask_full[b:h-b, b:w-b] = m2\n            \n            # Recalculate measurements with new mask\n            ys, xs = np.where(mask_full > 0)\n            if ys.size >= 5:\n                ymin, ymax = ys.min(), ys.max()\n                xmin, xmax = xs.min(), xs.max()\n                bbox_h = float((ymax - ymin + 1))\n                bbox_w = float((xmax - xmin + 1))\n                span = _pca_major_span(np.stack([ys, xs], 1))\n                height = max(bbox_h, span)\n                width = bbox_w\n                aspect = height / max(1.0, width)\n                dens = float(img_u8[mask_full > 0].mean())\n                area = float(mask_full.sum())\n                frac = area / float(h * w)\n        \n        # Return measurements and mask\n        return dict(\n            area=area,\n            height=float(np.clip(height, 0, h)),\n            width=float(np.clip(width, 0, w)),\n            aspect=float(aspect),\n            density=dens,\n            width_length_ratio=float(width / max(1.0, height)),\n            area_frac=frac\n        ), mask_full\n    except Exception as e:\n        print(f\"Warning: Mask generation failed: {e}\")\n        return dict(area=0.0, height=0.0, width=0.0, aspect=0.0, density=0.0, width_length_ratio=0.0), np.zeros_like(img_u8)\n\n# Function to compute robust measurements only (without mask)\ndef robust_measurements(img_u8: np.ndarray) -> dict:\n    d, _ = robust_measurements_and_mask(img_u8)\n    return d\n\n# Function to generate pediatric bone mask\ndef make_pediatric_mask(img_u8: np.ndarray) -> np.ndarray:\n    _, m = robust_measurements_and_mask(img_u8)\n    return m","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.328218Z","iopub.execute_input":"2025-10-02T16:19:24.328447Z","iopub.status.idle":"2025-10-02T16:19:24.345707Z","shell.execute_reply.started":"2025-10-02T16:19:24.328424Z","shell.execute_reply":"2025-10-02T16:19:24.345202Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**Clinical Metrics Calculation**","metadata":{}},{"cell_type":"code","source":"# Function to compute clinical metrics from bone image and mask\ndef compute_clinical_metrics(img_u8: np.ndarray, mask: np.ndarray) -> Dict[str, float]:\n    # Detect edges using Canny edge detector\n    edges = cv2.Canny(img_u8, 100, 200)\n    mask_bool = mask > 0  # Convert mask to boolean\n    \n    # Calculate mean edge value within bone mask (cortical thickness)\n    edge_in_bone = edges[mask_bool].mean() if mask_bool.sum() > 0 else 0.0\n    cortical_thickness = float(edge_in_bone / 255.0)\n    \n    # Define morphological kernel\n    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n    \n    # Calculate epiphyseal plate width by erosion\n    eroded = cv2.erode(mask, k)\n    plate_region = mask - eroded\n    plate_width = float(plate_region.sum() / mask.sum()) if mask.sum() > 0 else 0.0\n    \n    # Return clinical metrics\n    return {\n        \"cortical_thickness\": cortical_thickness,\n        \"epiphyseal_plate_width\": plate_width\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.346478Z","iopub.execute_input":"2025-10-02T16:19:24.346909Z","iopub.status.idle":"2025-10-02T16:19:24.359824Z","shell.execute_reply.started":"2025-10-02T16:19:24.346882Z","shell.execute_reply":"2025-10-02T16:19:24.359328Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"**Greulich-Pyle Bone Age Estimation**","metadata":{}},{"cell_type":"code","source":"# Function to estimate bone age using Greulich-Pyle method\ndef estimate_gp_bone_age(img_u8: np.ndarray, mask: np.ndarray, sex: int) -> float:\n    try:\n        # Compute clinical metrics\n        clinical = compute_clinical_metrics(img_u8, mask)\n        cortical_thickness = clinical[\"cortical_thickness\"]\n        epiphyseal_plate_width = clinical[\"epiphyseal_plate_width\"]\n        \n        # Set target age based on sex\n        target_age_m = 216 if sex == 1 else 204  # 18 years for males, 17 years for females\n        \n        # Define normal values for 18-year-old\n        cortical_norm = 0.7\n        plate_norm = 0.05\n        \n        # Calculate scores based on deviation from normal\n        cortical_score = 1.0 - abs(cortical_thickness - cortical_norm) / cortical_norm\n        plate_score = 1.0 - abs(epiphyseal_plate_width - plate_norm) / max(plate_norm, 1e-3)\n        \n        # Combine scores (weighted average)\n        combined_score = 0.6 * cortical_score + 0.4 * plate_score\n        \n        # Map score to age estimate\n        min_age, max_age = 144, 228  # 12 to 19 years in months\n        estimated_age = min_age + (max_age - min_age) * combined_score\n        estimated_age = max(min_age, min(max_age, estimated_age))\n        \n        # Adjust for sex differences\n        if sex == 0:  # Female\n            estimated_age = min(estimated_age, 204)\n            \n        return round(float(estimated_age), 1)  # Return rounded estimate\n    except Exception as e:\n        print(f\"Warning: GP bone age estimation failed: {e}\")\n        return float(target_age_m)  # Return target age if estimation fails","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.360799Z","iopub.execute_input":"2025-10-02T16:19:24.361066Z","iopub.status.idle":"2025-10-02T16:19:24.374817Z","shell.execute_reply.started":"2025-10-02T16:19:24.361050Z","shell.execute_reply":"2025-10-02T16:19:24.374335Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**Growth Schedule and Morphological Transformations**","metadata":{}},{"cell_type":"code","source":"# Dataclass to store growth schedule parameters\n@dataclass\nclass GrowthSchedule:\n    length_gain: float  # Proportional increase in length\n    width_gain: float   # Proportional increase in width\n    density_gain: float # Proportional increase in density\n    closure: float      # Degree of growth plate closure\n\n# Function to calculate growth schedule parameters based on age and sex\ndef schedule_params_for_age(age_m: int, sex: int, base: Dict[str, float]) -> GrowthSchedule:\n    a = max(0, min(216, int(age_m)))  # Clamp age to [0, 216] months\n    \n    # Set peak growth time based on sex\n    t_peak = 144 if sex == 0 else 164  # 12 years for females, 13.67 years for males\n    \n    # Calculate length and width gains using logistic functions\n    k_len, k_wid = 0.035, 0.028  # Growth rate constants\n    length_gain = 0.35 * (1 / (1 + math.exp(-k_len * (a - t_peak))))\n    width_gain = 0.18 * (1 / (1 + math.exp(-k_wid * (a - (t_peak - 10)))))\n    \n    # Calculate density gain using logistic function with sex-specific parameters\n    if sex == 0:  # Female parameters\n        L, x0, k, b = 0.564, 12.02, 0.591, 0.540\n    else:  # Male parameters\n        L, x0, k, b = 0.633, 13.65, 0.453, 0.514\n    \n    age_y = a / 12.0  # Convert to years\n    dens = b + L / (1 + math.exp(-k * (age_y - x0)))  # Logistic function\n    density_gain = 0.25 * (dens - b) / L  # Scale density gain\n    \n    # Calculate growth plate closure using logistic function\n    t_close_mid = 198 if sex == 0 else 222  # Midpoint of closure\n    k_close = 0.06  # Closure rate\n    closure = 1 / (1 + math.exp(-k_close * (a - t_close_mid)))\n    \n    # Add random variation to parameters\n    length_gain += random.normalvariate(0, 0.05)\n    length_gain = max(0.0, min(1.0, length_gain))\n    width_gain += random.normalvariate(0, 0.03)\n    width_gain = max(0.0, min(1.0, width_gain))\n    density_gain += random.normalvariate(0, 0.02)\n    density_gain = max(0.0, min(0.25, density_gain))\n    closure += random.normalvariate(0, 0.05)\n    closure = max(0.0, min(1.0, closure))\n    \n    return GrowthSchedule(length_gain, width_gain, density_gain, closure)\n\n# Function to morph pediatric bone image to adult-like appearance\ndef morph_adultize_at_age(base_img: Image.Image, sex: int, age_m: int) -> Image.Image:\n    # Convert to numpy array\n    u8 = np.array(base_img, dtype=np.uint8)\n    \n    # Get measurements and growth schedule\n    meas = robust_measurements(u8)\n    sch = schedule_params_for_age(age_m, sex, meas)\n    \n    # Convert to float [0,1]\n    f = from_uint8(u8)\n    \n    # Calculate scaling factors\n    sy = 1.0 + sch.length_gain  # Height scaling\n    sx = 1.0 + sch.width_gain * 0.6  # Width scaling (less than height)\n    \n    # Apply affine transformation for scaling\n    h, w = f.shape\n    M = np.array([[sx, 0, (1 - sx) * w / 2],  # Scaling matrix\n                  [0, sy, (1 - sy) * h / 2]], dtype=np.float32)\n    scaled = cv2.warpAffine(f, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n    \n    # Apply morphological closing to simulate growth plate closure\n    k = max(1, int(1 + 6 * sch.closure))\n    closed = cv2.morphologyEx(scaled, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k)))\n    \n    # Enhance cortical bone using Laplacian edges\n    edges = cv2.Laplacian(closed, cv2.CV_32F, ksize=3)\n    cortical = np.clip(closed + 0.15 * (sch.density_gain + sch.closure * 0.3) * edges, 0.0, 1.0)\n    \n    # Increase overall bone density\n    dense = np.clip(cortical * (1.0 + 0.35 * sch.density_gain), 0.0, 1.0)\n    \n    # Convert back to uint8 and apply bilateral filter for smoothing\n    u8a = to_uint8(dense)\n    u8a = cv2.bilateralFilter(u8a, 5, 30, 30)\n    \n    return Image.fromarray(u8a)  # Return as PIL image\n\n# Function to build parameter vector for conditioning\ndef build_param_vec(age_m: int, sex: int, base: Dict[str, float]) -> torch.Tensor:\n    # Get growth schedule\n    sch = schedule_params_for_age(age_m, sex, base)\n    \n    # Normalize base measurements\n    bl = base.get(\"height\", 0.0) / CFG.IMG_SIZE\n    bw = base.get(\"width\", 0.0) / CFG.IMG_SIZE\n    ar = base.get(\"aspect\", 0.0)\n    area_frac = base.get(\"area_frac\", base.get(\"area\", 0.0) / (CFG.IMG_SIZE * CFG.IMG_SIZE))\n    dens = (base.get(\"density\", 128.0) / 255.0)\n    \n    # Create parameter vector\n    vec = [\n        sch.length_gain, sch.width_gain, sch.density_gain, sch.closure,  # Growth parameters\n        bl, bw, ar, area_frac, dens,  # Normalized measurements\n        float(sex), age_m / 216.0,  # Sex and normalized age\n        0.0, 0.0, 0.0  # Padding\n    ]\n    \n    return torch.tensor(vec[:14], dtype=torch.float32)  # Return as tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.375614Z","iopub.execute_input":"2025-10-02T16:19:24.375982Z","iopub.status.idle":"2025-10-02T16:19:24.395293Z","shell.execute_reply.started":"2025-10-02T16:19:24.375959Z","shell.execute_reply":"2025-10-02T16:19:24.394594Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Dataset Class**","metadata":{}},{"cell_type":"code","source":"# Dataset class for bone age conditional diffusion\nclass BoneAgeCondDiffusionDS(Dataset):\n    def __init__(self, csv_path: str, img_dir: str, img_size: int = CFG.IMG_SIZE, max_samples: Optional[int] = CFG.MAX_SAMPLES):\n        # Load CSV data\n        self.df = pd.read_csv(csv_path).copy()\n        self.df[\"male\"] = self.df[\"male\"].astype(int)  # Convert male column to int\n        \n        # Index image directory\n        self.idx = index_image_dir(img_dir)\n        \n        # Filter dataframe to only include rows with images\n        keep = self.df[\"id\"].astype(int).isin(self.idx.keys())\n        self.df = self.df[keep].reset_index(drop=True)\n        \n        # Add image paths to dataframe\n        self.df[\"img_path\"] = self.df[\"id\"].astype(int).map(self.idx.get)\n        \n        # Limit number of samples if specified\n        if max_samples is not None and len(self.df) > max_samples:\n            self.df = self.df.sample(max_samples, random_state=CFG.SEED).reset_index(drop=True)\n        \n        self.size = img_size  # Image size\n        self.training = False  # Training mode flag\n        \n        # Define augmentation transforms\n        self.augment = transforms.Compose([\n            transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip\n            transforms.RandomRotation(degrees=10),   # Random rotation\n            transforms.RandomAffine(degrees=0, scale=(0.9, 1.1), translate=(0.05, 0.05)),  # Random scale and translation\n            transforms.ColorJitter(brightness=0.2, contrast=0.2)  # Random brightness and contrast\n        ])\n        \n        # Check if dataframe is empty\n        if len(self.df) == 0:\n            raise RuntimeError(\"No images matched IDs.\")\n    \n    def __len__(self): \n        return len(self.df)  # Return number of samples\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor | int]:\n        # Get row data\n        row = self.df.iloc[idx]\n        sex = int(row[\"male\"])\n        path = row[\"img_path\"]\n        \n        # Load and process base image\n        base_im = load_gray_png_any(path, self.size)\n        if self.training:\n            base_im = self.augment(base_im)  # Apply augmentations if in training mode\n        \n        # Convert to numpy array\n        base_u8 = np.array(base_im, dtype=np.uint8)\n        \n        # Get measurements and mask\n        base_meas, mask_full = robust_measurements_and_mask(base_u8)\n        \n        # Generate target image (adult-like)\n        tgt_im = morph_adultize_at_age(base_im, sex, CFG.TARGET_AGE)\n        if self.training:\n            tgt_im = self.augment(tgt_im)  # Apply augmentations if in training mode\n        \n        # Convert images to tensors\n        pedi_t = pil_to_tensor_gray(base_im).squeeze(0)  # Pediatric image tensor\n        mask_t = torch.from_numpy((mask_full.astype(np.float32)/255.0))[None, :, :]  # Mask tensor\n        mask_t = mask_t * 2.0 - 1.0  # Normalize to [-1,1]\n        adult_t = pil_to_tensor_gray(tgt_im).squeeze(0)  # Adult image tensor\n        \n        # Build parameter vector\n        params = build_param_vec(CFG.TARGET_AGE, sex, base_meas)\n        \n        # Return sample dictionary\n        return {\n            \"pediatric_img\": pedi_t,\n            \"pediatric_mask\": mask_t,\n            \"adult_img\": adult_t,\n            \"params\": params,\n            \"sex\": sex,\n            \"id\": int(row[\"id\"])\n        }\n    \n    def train(self):\n        self.training = True  # Enable training mode\n    \n    def eval(self):\n        self.training = False  # Disable training mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.396107Z","iopub.execute_input":"2025-10-02T16:19:24.396353Z","iopub.status.idle":"2025-10-02T16:19:24.413990Z","shell.execute_reply.started":"2025-10-02T16:19:24.396337Z","shell.execute_reply":"2025-10-02T16:19:24.413285Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Model Architecture with Attention**","metadata":{}},{"cell_type":"code","source":"# Cross-Attention module for incorporating condition information\nclass CrossAttention(nn.Module):\n    def __init__(self, in_ch: int, cdim: int, heads: int = CFG.ATTN_HEADS, dim_head: int = CFG.ATTN_DIM):\n        super().__init__()\n        self.heads = heads  # Number of attention heads\n        self.scale = dim_head ** -0.5  # Scaling factor for attention\n        \n        # Linear layers for queries, keys, and values\n        self.to_q = nn.Linear(in_ch, heads * dim_head, bias=False)\n        self.to_k = nn.Linear(cdim, heads * dim_head, bias=False)\n        self.to_v = nn.Linear(cdim, heads * dim_head, bias=False)\n        \n        # Output projection\n        self.to_out = nn.Linear(heads * dim_head, in_ch)\n        \n        # Normalization layer\n        self.norm = nn.GroupNorm(8, in_ch)\n\n    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n        B, C, H, W = x.shape  # Get dimensions\n        \n        # Normalize input\n        x_norm = self.norm(x)\n        \n        # Flatten spatial dimensions\n        x_flat = x_norm.permute(0, 2, 3, 1).reshape(B, H*W, C)  # [B, HW, C]\n        \n        # Compute queries, keys, and values\n        q = self.to_q(x_flat).view(B, H*W, self.heads, -1).permute(0, 2, 1, 3)  # [B, heads, HW, dim_head]\n        k = self.to_k(context).view(B, 1, self.heads, -1).permute(0, 2, 1, 3)   # [B, heads, 1, dim_head]\n        v = self.to_v(context).view(B, 1, self.heads, -1).permute(0, 2, 1, 3)   # [B, heads, 1, dim_head]\n        \n        # Compute attention scores\n        attn = (q @ k.transpose(-2, -1)) * self.scale  # [B, heads, HW, 1]\n        attn = F.softmax(attn, dim=-1)  # Normalize attention scores\n        \n        # Apply attention to values\n        out = (attn @ v).permute(0, 2, 1, 3).reshape(B, H*W, -1)  # [B, HW, heads*dim_head]\n        \n        # Project output and reshape\n        out = self.to_out(out).reshape(B, C, H, W)  # [B, C, H, W]\n        \n        return x + out  # Residual connection\n\n# Sinusoidal time embedding\nclass SinusoidalTime(nn.Module):\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim  # Embedding dimension\n    \n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        half = self.dim // 2  # Half dimension for sin and cos\n        \n        # Compute frequencies\n        freqs = torch.exp(torch.arange(half, device=t.device) * -(math.log(10000.0) / (half - 1)))\n        \n        # Compute arguments for sin and cos\n        args = t[:, None] * freqs[None, :]\n        \n        # Compute sin and cos embeddings\n        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n        \n        # Pad if dimension is odd\n        if self.dim % 2 == 1: \n            emb = F.pad(emb, (0, 1))\n            \n        return emb\n\n# FiLM (Feature-wise Linear Modulation) layer\nclass FiLM(nn.Module):\n    def __init__(self, in_dim: int, ch: int):\n        super().__init__()\n        # Network to compute scale and shift\n        self.net = nn.Sequential(nn.Linear(in_dim, 128), nn.SiLU(), nn.Linear(128, 2 * ch))\n    \n    def forward(self, x: torch.Tensor, c: torch.Tensor):\n        # Compute scale and shift\n        gb = self.net(c)\n        g, b = gb.chunk(2, dim=-1)\n        \n        # Reshape for broadcasting\n        g = g.unsqueeze(-1).unsqueeze(-1)\n        b = b.unsqueeze(-1).unsqueeze(-1)\n        \n        # Apply FiLM transformation\n        return x * (1 + g) + b\n\n# Residual block with time and condition embeddings\nclass ResBlock(nn.Module):\n    def __init__(self, in_ch: int, out_ch: int, tdim: int, cdim: int):\n        super().__init__()\n        # Normalization and activation\n        self.norm1 = nn.GroupNorm(8, in_ch)\n        self.act1 = nn.SiLU()\n        \n        # First convolution\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        \n        # Time embedding projection\n        self.time = nn.Sequential(nn.SiLU(), nn.Linear(tdim, out_ch))\n        \n        # Condition embedding projection (FiLM)\n        self.cond = FiLM(cdim, out_ch)\n        \n        # Second normalization and activation\n        self.norm2 = nn.GroupNorm(8, out_ch)\n        self.act2 = nn.SiLU()\n        \n        # Second convolution\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        \n        # Skip connection if channels change\n        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n    \n    def forward(self, x, t, c):\n        # First block\n        h = self.conv1(self.act1(self.norm1(x)))\n        \n        # Add time embedding\n        h = h + self.time(t)[:, :, None, None]\n        \n        # Apply condition (FiLM)\n        h = self.cond(h, c)\n        \n        # Second block\n        h = self.conv2(self.act2(self.norm2(h)))\n        \n        # Skip connection\n        return h + self.skip(x)\n\n# Downsampling block\nclass Down(nn.Module):\n    def __init__(self, in_ch, out_ch, tdim, cdim):\n        super().__init__()\n        # Two residual blocks\n        self.b1 = ResBlock(in_ch, out_ch, tdim, cdim)\n        self.b2 = ResBlock(out_ch, out_ch, tdim, cdim)\n        \n        # Downsampling convolution\n        self.pool = nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1)\n    \n    def forward(self, x, t, c):\n        # Apply residual blocks\n        x = self.b1(x, t, c)\n        x = self.b2(x, t, c)\n        \n        # Save skip connection\n        skip = x\n        \n        # Downsample\n        x = self.pool(x)\n        \n        return x, skip\n\n# Upsampling block\nclass Up(nn.Module):\n    def __init__(self, in_ch, skip_ch, out_ch, tdim, cdim, use_attn=False):\n        super().__init__()\n        # Upsampling convolution\n        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n        \n        # Two residual blocks\n        self.b1 = ResBlock(out_ch + skip_ch, out_ch, tdim, cdim)\n        self.b2 = ResBlock(out_ch, out_ch, tdim, cdim)\n        \n        # Optional attention\n        self.attn = CrossAttention(out_ch, cdim, CFG.ATTN_HEADS, CFG.ATTN_DIM) if use_attn else None\n    \n    def forward(self, x, skip, t, c):\n        # Upsample\n        x = self.up(x)\n        \n        # Concatenate with skip connection\n        x = torch.cat([x, skip], dim=1)\n        \n        # Apply residual blocks\n        x = self.b1(x, t, c)\n        x = self.b2(x, t, c)\n        \n        # Apply attention if enabled\n        if self.attn is not None:\n            x = self.attn(x, c)\n        \n        return x\n\n# Conditional U-Net model\nclass CondUNet(nn.Module):\n    def __init__(self, cvec_dim=14, base_ch=64):  # Reduced base channels for GPU limits\n        super().__init__()\n        \n        # Time embedding dimension\n        tdim = base_ch * 4\n        \n        # Time embedding network\n        self.time = nn.Sequential(SinusoidalTime(tdim), nn.Linear(tdim, tdim), nn.SiLU(), nn.Linear(tdim, tdim))\n        \n        # Condition vector projection\n        self.cproj = nn.Sequential(nn.Linear(cvec_dim, 128), nn.SiLU(), nn.Linear(128, 128))\n        cdim = 128  # Condition dimension after projection\n        \n        # Input convolution\n        self.inp = nn.Conv2d(3, base_ch, 3, padding=1)  # 3 channels: noisy + pediatric + mask\n        \n        # Downsampling blocks\n        self.d1 = Down(base_ch, base_ch, tdim, cdim)\n        self.d2 = Down(base_ch, base_ch*2, tdim, cdim)\n        self.d3 = Down(base_ch*2, base_ch*4, tdim, cdim)\n        self.d4 = Down(base_ch*4, base_ch*4, tdim, cdim)\n        \n        # Middle blocks\n        self.mid1 = ResBlock(base_ch*4, base_ch*4, tdim, cdim)\n        self.mid_attn = CrossAttention(base_ch*4, cdim, CFG.ATTN_HEADS, CFG.ATTN_DIM)\n        self.mid2 = ResBlock(base_ch*4, base_ch*4, tdim, cdim)\n        \n        # Upsampling blocks\n        self.u4 = Up(base_ch*4, base_ch*4, base_ch*4, tdim, cdim, use_attn=True)\n        self.u3 = Up(base_ch*4, base_ch*4, base_ch*2, tdim, cdim, use_attn=True)\n        self.u2 = Up(base_ch*2, base_ch*2, base_ch, tdim, cdim)\n        self.u1 = Up(base_ch, base_ch, base_ch, tdim, cdim)\n        \n        # Output normalization and convolution\n        self.out_norm = nn.GroupNorm(8, base_ch)\n        self.out = nn.Conv2d(base_ch, 1, 3, padding=1)  # Single channel output\n\n    def forward(self, x_noisy: torch.Tensor, cond_2ch: torch.Tensor, t_idx: torch.Tensor, cvec: torch.Tensor):\n        # Compute time and condition embeddings\n        t = self.time(t_idx.float())\n        c = self.cproj(cvec)\n        \n        # Concatenate noisy image with condition (pediatric + mask)\n        x = torch.cat([x_noisy, cond_2ch], dim=1)\n        \n        # Apply input convolution\n        x = self.inp(x)\n        \n        # Apply downsampling blocks\n        x, s1 = self.d1(x, t, c)\n        x, s2 = self.d2(x, t, c)\n        x, s3 = self.d3(x, t, c)\n        x, s4 = self.d4(x, t, c)\n        \n        # Apply middle blocks\n        x = self.mid1(x, t, c)\n        x = self.mid_attn(x, c)\n        x = self.mid2(x, t, c)\n        \n        # Apply upsampling blocks\n        x = self.u4(x, s4, t, c)\n        x = self.u3(x, s3, t, c)\n        x = self.u2(x, s2, t, c)\n        x = self.u1(x, s1, t, c)\n        \n        # Apply output normalization and activation\n        x = F.silu(self.out_norm(x))\n        \n        # Return output\n        return self.out(x)\n\n# Function to load pretrained U-Net weights\ndef load_pretrained_unet(net: CondUNet, pretrained_model=\"google/ddpm-cifar10-32\"):\n    try:\n        # Load pretrained model\n        pretrained = UNet2DModel.from_pretrained(pretrained_model).to(DEVICE)\n        state_dict = pretrained.state_dict()\n        \n        # Get model state dict\n        net_dict = net.state_dict()\n        \n        # Copy matching weights\n        for k, v in state_dict.items():\n            if k in net_dict and v.shape == net_dict[k].shape:\n                net_dict[k] = v\n        \n        # Load weights\n        net.load_state_dict(net_dict, strict=False)\n        print(f\"Loaded pretrained weights from {pretrained_model}\")\n    except Exception as e:\n        print(f\"Failed to load pretrained weights: {e}\")\n    \n    return net","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.416375Z","iopub.execute_input":"2025-10-02T16:19:24.416565Z","iopub.status.idle":"2025-10-02T16:19:24.443177Z","shell.execute_reply.started":"2025-10-02T16:19:24.416551Z","shell.execute_reply":"2025-10-02T16:19:24.442519Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"**Diffusion Scheduler**","metadata":{}},{"cell_type":"code","source":"# Dataclass for diffusion scheduler parameters\n@dataclass\nclass DiffSched:\n    betas: torch.Tensor      # Noise schedule\n    alphas: torch.Tensor      # 1 - betas\n    ac: torch.Tensor         # Cumulative product of alphas\n    sqrt_ac: torch.Tensor    # Square root of ac\n    sqrt_om: torch.Tensor    # Square root of 1 - ac\n\n# Function to create diffusion scheduler\ndef make_scheduler(T: int = CFG.T_STEPS, beta_start=1e-4, beta_end=0.02) -> DiffSched:\n    # Linear noise schedule\n    betas = torch.linspace(beta_start, beta_end, T, dtype=torch.float32, device=DEVICE)\n    \n    # Compute alphas and cumulative products\n    alphas = 1.0 - betas\n    ac = torch.cumprod(alphas, dim=0)\n    \n    # Return scheduler\n    return DiffSched(\n        betas=betas, \n        alphas=alphas, \n        ac=ac,\n        sqrt_ac=torch.sqrt(ac), \n        sqrt_om=torch.sqrt(1.0 - ac),\n    )\n\n# Create global scheduler\nSCHED = make_scheduler()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.443897Z","iopub.execute_input":"2025-10-02T16:19:24.444270Z","iopub.status.idle":"2025-10-02T16:19:24.724130Z","shell.execute_reply.started":"2025-10-02T16:19:24.444253Z","shell.execute_reply":"2025-10-02T16:19:24.723529Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"**Loss Function**","metadata":{}},{"cell_type":"code","source":"# Function to compute soft area fraction\ndef _soft_area_frac(x0_hat: torch.Tensor, k: float = CFG.SOFT_K) -> torch.Tensor:\n    s = torch.sigmoid(k * x0_hat)  # Apply sigmoid with scaling\n    return s.mean(dim=[1,2,3])  # Mean over spatial dimensions\n\n# Function to compute expected area fraction from condition vector\ndef _expected_area_frac(cvec: torch.Tensor) -> torch.Tensor:\n    # Extract gains from condition vector\n    len_gain = cvec[:, 0]\n    wid_gain = cvec[:, 1]\n    base_area_frac = cvec[:, 7].clamp(0.0, 1.0)\n    \n    # Compute expected area with scaling\n    scale = (1.0 + len_gain).clamp(0.5, 2.0) * (1.0 + 0.6 * wid_gain).clamp(0.5, 2.0)\n    exp = (base_area_frac * scale).clamp(CFG.AREA_MIN, CFG.AREA_MAX)\n    \n    return exp\n\n# DDPM loss function\ndef ddpm_loss_step(net: CondUNet, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n    # Get batch data\n    x0 = batch[\"adult_img\"].to(DEVICE)  # Target image\n    pedi = batch[\"pediatric_img\"].to(DEVICE)  # Pediatric image\n    mask = batch[\"pediatric_mask\"].to(DEVICE)  # Mask\n    cond = torch.cat([pedi, mask], dim=1)  # Concatenate condition\n    cvec = batch[\"params\"].to(DEVICE)  # Parameter vector\n    \n    B = x0.size(0)  # Batch size\n    \n    # Sample random timesteps\n    t = torch.randint(0, CFG.T_STEPS, (B,), device=DEVICE)\n    \n    # Sample noise\n    eps = torch.randn_like(x0)\n    \n    # Get scheduler values for timesteps\n    sqrt_ac = SCHED.sqrt_ac[t].view(B,1,1,1)\n    sqrt_om = SCHED.sqrt_om[t].view(B,1,1,1)\n    \n    # Forward diffusion: add noise to image\n    x_t = sqrt_ac * x0 + sqrt_om * eps\n    \n    # Classifier-free guidance: randomly drop condition\n    drop = (torch.rand(B, device=DEVICE) < CFG.CFG_DROP).float().view(B,1,1,1)\n    cond_cf = (1 - drop) * cond\n    cvec_cf = (1 - drop[:,0,0,0])[:,None] * cvec\n    \n    # Predict noise\n    eps_pred = net(x_t, cond_cf, t, cvec_cf)\n    \n    # Compute simple MSE loss\n    l_simple = F.mse_loss(eps_pred, eps)\n    \n    # Compute area loss\n    with torch.no_grad():\n        ac_t = SCHED.ac[t].view(B,1,1,1)\n    \n    # Predict x0 from noise\n    x0_hat = (x_t - torch.sqrt(1 - ac_t) * eps_pred) / torch.sqrt(ac_t + 1e-8)\n    \n    # Compute predicted area\n    area_pred = _soft_area_frac(x0_hat, CFG.SOFT_K)\n    \n    # Compute expected area\n    area_exp = _expected_area_frac(cvec)\n    \n    # Compute area loss\n    l_area = F.mse_loss(area_pred, area_exp)\n    \n    # Return combined loss\n    return l_simple + CFG.AREA_LAMBDA * l_area","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.724947Z","iopub.execute_input":"2025-10-02T16:19:24.725291Z","iopub.status.idle":"2025-10-02T16:19:24.735573Z","shell.execute_reply.started":"2025-10-02T16:19:24.725262Z","shell.execute_reply":"2025-10-02T16:19:24.734833Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**DDIM Sampler**","metadata":{}},{"cell_type":"code","source":"# DDIM sampling function\n@torch.no_grad()\ndef ddim_sample(net: CondUNet, pedi: torch.Tensor, mask: torch.Tensor, cvec: torch.Tensor, \n                steps: int = CFG.DDIM_STEPS, eta: float = 0.0, cfg_w: float = CFG.CFG_WEIGHT) -> torch.Tensor:\n    # Set model to evaluation mode\n    net.eval()\n    \n    # Get timestep parameters\n    T = CFG.T_STEPS\n    ts = torch.linspace(T-1, 0, steps, dtype=torch.long, device=DEVICE)  # Timesteps\n    \n    # Initialize with random noise\n    x = torch.randn_like(pedi)\n    \n    # Concatenate pediatric image and mask\n    cond = torch.cat([pedi, mask], dim=1)\n    \n    # Define function to predict noise with classifier-free guidance\n    def eps_hat(x, t, c_in, p_in):\n        # Predict noise with condition\n        e_cond = net(x, c_in, t, p_in)\n        \n        # Predict noise without condition\n        e_uncond = net(x, torch.zeros_like(c_in), t, torch.zeros_like(p_in))\n        \n        # Combine predictions with guidance weight\n        return e_uncond + cfg_w * (e_cond - e_uncond)\n    \n    # Set up automatic mixed precision context\n    scaler_ctx = torch.amp.autocast('cuda', enabled=CFG.AMP)\n    \n    # Iteratively denoise\n    for i in range(steps):\n        # Current timestep\n        t = ts[i].repeat(pedi.size(0))\n        \n        # Get scheduler values\n        ac_t = SCHED.ac[t].view(-1,1,1,1)\n        ac_prev = SCHED.ac[ts[i+1]].view(-1,1,1,1) if i+1 < steps else torch.ones_like(ac_t)\n        \n        # Predict noise\n        with scaler_ctx:\n            eps = eps_hat(x, t, cond, cvec)\n        \n        # Predict x0\n        x0_hat = (x - torch.sqrt(1 - ac_t) * eps) / torch.sqrt(ac_t + 1e-8)\n        \n        # DDIM update step\n        if eta == 0.0:  # Deterministic\n            x = torch.sqrt(ac_prev) * x0_hat + torch.sqrt(1 - ac_prev) * eps\n        else:  # Stochastic\n            sigma = eta * torch.sqrt((1 - ac_prev) / (1 - ac_t) * (1 - ac_t / ac_prev))\n            noise = torch.randn_like(x)\n            x = torch.sqrt(ac_prev) * x0_hat + torch.sqrt(torch.clamp(1 - ac_prev - sigma**2, 0.0)) * eps + sigma * noise\n    \n    # Clamp to valid range and return\n    return x.clamp(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.736365Z","iopub.execute_input":"2025-10-02T16:19:24.736651Z","iopub.status.idle":"2025-10-02T16:19:24.752395Z","shell.execute_reply.started":"2025-10-02T16:19:24.736635Z","shell.execute_reply":"2025-10-02T16:19:24.751754Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"**Save/Load Helpers**","metadata":{}},{"cell_type":"code","source":"# Function to unwrap state dict for saving (handles compiled models)\ndef unwrap_state_dict_for_save(model: torch.nn.Module) -> Dict[str, torch.Tensor]:\n    m = model\n    \n    # Handle compiled models\n    if hasattr(m, \"_orig_mod\") and isinstance(getattr(m, \"_orig_mod\"), torch.nn.Module):\n        m = m._orig_mod\n    \n    # Handle DataParallel models\n    if hasattr(m, \"module\") and isinstance(getattr(m, \"module\"), torch.nn.Module):\n        m = m.module\n    \n    return m.state_dict()\n\n# Function to clean state dict prefixes (for loading)\ndef clean_state_dict_prefixes(state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    # Handle compiled model prefixes\n    if any(k.startswith(\"_orig_mod.\") for k in state.keys()):\n        state = {k.replace(\"_orig_mod.\", \"\", 1): v for k, v in state.items()}\n    \n    # Handle DataParallel prefixes\n    if any(k.startswith(\"module.\") for k in state.keys()):\n        state = {k.replace(\"module.\", \"\", 1): v for k, v in state.items()}\n    \n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.753082Z","iopub.execute_input":"2025-10-02T16:19:24.753315Z","iopub.status.idle":"2025-10-02T16:19:24.768945Z","shell.execute_reply.started":"2025-10-02T16:19:24.753294Z","shell.execute_reply":"2025-10-02T16:19:24.768253Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"**Metrics Calculation**","metadata":{}},{"cell_type":"code","source":"# Function to compute PSNR (Peak Signal-to-Noise Ratio)\ndef compute_psnr(pred_u8: np.ndarray, tgt_u8: np.ndarray) -> float:\n    # Compute MSE\n    mse = float(np.mean((pred_u8.astype(np.float32) - tgt_u8.astype(np.float32)) ** 2))\n    \n    # Handle perfect match case\n    if mse <= 1e-12: \n        return 99.0\n    \n    # Compute PSNR\n    return 20.0 * math.log10(255.0) - 10.0 * math.log10(mse)\n\n# Function to compute SSIM (Structural Similarity Index)\ndef compute_ssim(pred_u8: np.ndarray, tgt_u8: np.ndarray) -> float:\n    # Use skimage implementation if available\n    if skimage_ssim is not None:\n        return float(skimage_ssim(pred_u8, tgt_u8, data_range=255))\n    \n    # Fallback to manual implementation\n    pred = pred_u8.astype(np.float32)\n    tgt = tgt_u8.astype(np.float32)\n    \n    # Constants for SSIM calculation\n    C1, C2 = (0.01 * 255)**2, (0.03 * 255)**2\n    \n    # Gaussian kernel\n    k = 11\n    g = cv2.getGaussianKernel(k, 1.5)\n    w = g @ g.T\n    \n    # Compute means\n    mu1 = cv2.filter2D(pred, -1, w)\n    mu2 = cv2.filter2D(tgt, -1, w)\n    \n    # Compute squares of means\n    mu1_sq, mu2_sq = mu1*mu1, mu2*mu2\n    mu1_mu2 = mu1*mu2\n    \n    # Compute variances and covariance\n    sigma1_sq = cv2.filter2D(pred*pred, -1, w) - mu1_sq\n    sigma2_sq = cv2.filter2D(tgt*tgt, -1, w) - mu2_sq\n    sigma12 = cv2.filter2D(pred*tgt, -1, w) - mu1_mu2\n    \n    # Compute SSIM\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2) + 1e-12)\n    \n    return float(ssim_map.mean())\n\n# Function to compute MS-SSIM (Multi-Scale SSIM)\ndef compute_msssim(pred_u8: np.ndarray, tgt_u8: np.ndarray) -> Optional[float]:\n    # Return None if torch_ms_ssim is not available\n    if torch_ms_ssim is None:\n        return None\n    \n    # Convert to tensors\n    pred = torch.from_numpy(pred_u8).float().unsqueeze(0).unsqueeze(0) / 255.0\n    tgt = torch.from_numpy(tgt_u8).float().unsqueeze(0).unsqueeze(0) / 255.0\n    \n    # Compute MS-SSIM\n    with torch.no_grad():\n        return float(torch_ms_ssim(pred, tgt, data_range=1.0).item())\n\n# Function to compute LPIPS (Learned Perceptual Image Patch Similarity)\ndef compute_lpips(pred_u8: np.ndarray, tgt_u8: np.ndarray, lpips_alex=None) -> Optional[float]:\n    # Return None if lpips is not available\n    if lpips is None:\n        return None\n    \n    try:\n        # Initialize LPIPS model if not provided\n        if lpips_alex is None:\n            lpips_alex = lpips.LPIPS(net='alex').to(DEVICE)\n        \n        # Prepare images for LPIPS\n        def prep(x):\n            t = torch.from_numpy(x).float()/255.0\n            t = (t*2.0 - 1.0).unsqueeze(0).unsqueeze(0).repeat(1,3,1,1).to(DEVICE)\n            return t\n        \n        # Compute LPIPS\n        with torch.no_grad():\n            d = lpips_alex(prep(pred_u8), prep(tgt_u8))\n            return float(d.item())\n    except Exception:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.769619Z","iopub.execute_input":"2025-10-02T16:19:24.769896Z","iopub.status.idle":"2025-10-02T16:19:24.785623Z","shell.execute_reply.started":"2025-10-02T16:19:24.769869Z","shell.execute_reply":"2025-10-02T16:19:24.785063Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"**Training Function**","metadata":{}},{"cell_type":"code","source":"# Training function with automatic mixed precision and gradient accumulation\ndef train_diffusion_amp_accum(\n    net: CondUNet,\n    train_loader: DataLoader,\n    val_loader: Optional[DataLoader] = None,\n    epochs: int = CFG.EPOCHS,\n    save_path: str = os.path.join(CFG.SAVE_DIR, \"cond_diffusion_mask_prior.pth\")\n):\n    # Move model to device\n    net = net.to(DEVICE)\n    \n    # Compile model if enabled\n    if CFG.COMPILE and hasattr(torch, \"compile\"):\n        try: \n            net = torch.compile(net)\n        except Exception: \n            pass\n    \n    # Initialize optimizer and scheduler\n    opt = torch.optim.AdamW(net.parameters(), lr=CFG.LR, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-6)\n    \n    # Initialize gradient scaler for mixed precision\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG.AMP and DEVICE.type == \"cuda\")\n    \n    # Track best validation loss\n    best = float(\"inf\")\n    \n    # Print training configuration\n    print(f\"AMP={CFG.AMP}, ACCUM_STEPS={CFG.ACCUM_STEPS}, eff_batch={CFG.BATCH*CFG.ACCUM_STEPS}\")\n    \n    # Training loop\n    for ep in range(1, epochs + 1):\n        # Set model to training mode\n        net.train()\n        \n        # Initialize loss and accumulation counter\n        tot, acc = 0.0, 0\n        \n        # Create progress bar\n        pbar = tqdm(train_loader, desc=f\"Train Ep {ep}/{epochs}\")\n        \n        # Zero gradients\n        opt.zero_grad(set_to_none=True)\n        \n        # Iterate over batches\n        for it, batch in enumerate(pbar, 1):\n            # Forward pass with mixed precision\n            with torch.amp.autocast('cuda', enabled=CFG.AMP):\n                loss = ddpm_loss_step(net, batch) / CFG.ACCUM_STEPS\n            \n            # Backward pass\n            scaler.scale(loss).backward()\n            acc += 1\n            \n            # Update weights if accumulation steps reached or end of epoch\n            if acc == CFG.ACCUM_STEPS or it == len(train_loader):\n                # Unscale gradients\n                scaler.unscale_(opt)\n                \n                # Clip gradients\n                torch.nn.utils.clip_grad_norm_(net.parameters(), CFG.CLIP_NORM)\n                \n                # Update weights\n                scaler.step(opt)\n                scaler.update()\n                \n                # Zero gradients\n                opt.zero_grad(set_to_none=True)\n                acc = 0\n            \n            # Update total loss\n            tot += float(loss.detach().cpu()) * CFG.ACCUM_STEPS\n            \n            # Update progress bar\n            pbar.set_postfix({\"loss\": f\"{(tot/it):.4f}\", \"lr\": f\"{opt.param_groups[0]['lr']:.6f}\"})\n        \n        # Update learning rate\n        scheduler.step()\n        \n        # Calculate average training loss\n        tl = tot / max(1, len(train_loader))\n        \n        # Validation\n        if val_loader is not None:\n            # Set model to evaluation mode\n            net.eval()\n            \n            # Initialize validation loss\n            vtot = 0.0\n            \n            # Iterate over validation batches\n            with torch.no_grad(), torch.amp.autocast('cuda', enabled=CFG.AMP):\n                for vb in val_loader:\n                    vtot += float(ddpm_loss_step(net, vb).detach().cpu())\n            \n            # Calculate average validation loss\n            vl = vtot / max(1, len(val_loader))\n        else:\n            vl = tl\n        \n        # Print epoch results\n        print(f\"[Epoch {ep}] train={tl:.4f} val={vl:.4f} lr={opt.param_groups[0]['lr']:.6f}\")\n        \n        # Save best model\n        if vl < best:\n            best = vl\n            clean_sd = unwrap_state_dict_for_save(net)\n            torch.save({\"model\": clean_sd}, save_path)\n            print(f\"Saved {save_path} (best {best:.4f})\")\n    \n    return save_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.786333Z","iopub.execute_input":"2025-10-02T16:19:24.786517Z","iopub.status.idle":"2025-10-02T16:19:24.801216Z","shell.execute_reply.started":"2025-10-02T16:19:24.786502Z","shell.execute_reply":"2025-10-02T16:19:24.800547Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"**Inference and Export Function**","metadata":{}},{"cell_type":"code","source":"# Function to run inference and export results\n@torch.no_grad()\ndef infer_val_and_export(\n    net_ckpt: str,\n    ds: BoneAgeCondDiffusionDS,\n    val_indices: List[int],\n    out_dir: str = CFG.OUT_DIR,\n    excel_path: str = CFG.MEAS_XLSX\n):\n    # Create output directory\n    os.makedirs(out_dir, exist_ok=True)\n    \n    # Initialize model\n    net = CondUNet(cvec_dim=14).to(DEVICE)\n    \n    # Load checkpoint\n    sd = torch.load(net_ckpt, map_location=DEVICE)\n    raw = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n    raw = clean_state_dict_prefixes(raw)\n    net.load_state_dict(raw, strict=False)\n    \n    # Initialize LPIPS model\n    lpips_alex = None\n    if lpips is not None:\n        try:\n            lpips_alex = lpips.LPIPS(net='alex').to(DEVICE).eval()\n        except Exception:\n            lpips_alex = None\n    \n    # Try to load font\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 16)\n    except Exception:\n        font = ImageFont.load_default()\n    \n    # Initialize records and paths\n    records, paths = [], []\n    \n    # Iterate over validation indices\n    for i in tqdm(val_indices, desc=\"Infer+Export (val)\"):\n        # Get row data\n        row = ds.df.iloc[i]\n        path = row[\"img_path\"]\n        sex = int(row[\"male\"])\n        boneage = int(row.get(\"boneage\", -1))\n        \n        # Extract image ID\n        img_id = int(_ID_RE.search(os.path.basename(path)).group(1)) if _ID_RE.search(os.path.basename(path)) else i\n        sex_str = \"Male\" if sex == 1 else \"Female\"\n        \n        # Load and process image\n        base_im = load_gray_png_any(path, CFG.IMG_SIZE)\n        base_u8 = np.array(base_im, dtype=np.uint8)\n        \n        # Get measurements and mask\n        pedi_meas, mask_full = robust_measurements_and_mask(base_u8)\n        \n        # Convert to tensors\n        pedi = pil_to_tensor_gray(base_im).to(DEVICE)\n        mask = torch.from_numpy((mask_full.astype(np.float32)/255.0))[None,None,:,:].to(DEVICE) * 2.0 - 1.0\n        \n        # Build parameter vector\n        cvec = build_param_vec(CFG.TARGET_AGE, sex, pedi_meas).unsqueeze(0).to(DEVICE)\n        \n        # Generate prediction\n        pred = ddim_sample(net, pedi, mask, cvec, steps=CFG.DDIM_STEPS, eta=0.0, cfg_w=CFG.CFG_WEIGHT)\n        pred_im = tensor_to_pil_gray(pred)\n        pred_u8 = np.array(pred_im, dtype=np.uint8)\n        \n        # Get measurements for prediction\n        pred_meas, pred_mask = robust_measurements_and_mask(pred_u8)\n        \n        # Generate target image\n        tgt_im = morph_adultize_at_age(base_im, sex, CFG.TARGET_AGE)\n        tgt_u8 = np.array(tgt_im, dtype=np.uint8)\n        \n        # Compute metrics\n        psnr = compute_psnr(pred_u8, tgt_u8)\n        ssim_val = compute_ssim(pred_u8, tgt_u8)\n        msssim_val = compute_msssim(pred_u8, tgt_u8)\n        lpips_val = compute_lpips(pred_u8, tgt_u8, lpips_alex)\n        \n        # Compute clinical metrics\n        pedi_clinical = compute_clinical_metrics(base_u8, mask_full)\n        pred_clinical = compute_clinical_metrics(pred_u8, pred_mask)\n        \n        # Estimate GP bone age\n        pred_gp_bone_age = estimate_gp_bone_age(pred_u8, pred_mask, sex)\n        \n        # Create composite image\n        composite = Image.new('L', (2 * CFG.IMG_SIZE, CFG.IMG_SIZE + 30))\n        composite.paste(base_im, (0, 30))\n        composite.paste(pred_im, (CFG.IMG_SIZE, 30))\n        \n        # Add text annotations\n        draw = ImageDraw.Draw(composite)\n        draw.text((10, 5), f\"Pediatric: ID={img_id}, Age={boneage}m, {sex_str}\", fill=255, font=font)\n        draw.text((CFG.IMG_SIZE + 10, 5), f\"Predicted: 216m (GP est: {pred_gp_bone_age}m)\", fill=255, font=font)\n        \n        # Save composite image\n        fp = os.path.join(out_dir, f\"{img_id}_{boneage}m_{sex_str}_vs_predicted_216m.png\")\n        composite.save(fp)\n        paths.append(fp)\n        \n        # Add record\n        records.append({\n            \"id\": img_id, \"boneage\": boneage, \"sex\": sex_str,\n            \"pediatric_area\": pedi_meas[\"area\"], \"pediatric_height\": pedi_meas[\"height\"],\n            \"pediatric_width\": pedi_meas[\"width\"], \"pediatric_aspect\": pedi_meas[\"aspect\"],\n            \"pediatric_density\": pedi_meas[\"density\"], \"pediatric_width_length_ratio\": pedi_meas[\"width_length_ratio\"],\n            \"pediatric_cortical_thickness\": pedi_clinical[\"cortical_thickness\"],\n            \"pediatric_epiphyseal_plate_width\": pedi_clinical[\"epiphyseal_plate_width\"],\n            \"predicted_area\": pred_meas[\"area\"], \"predicted_height\": pred_meas[\"height\"],\n            \"predicted_width\": pred_meas[\"width\"], \"predicted_aspect\": pred_meas[\"aspect\"],\n            \"predicted_density\": pred_meas[\"density\"], \"predicted_width_length_ratio\": pred_meas[\"width_length_ratio\"],\n            \"predicted_cortical_thickness\": pred_clinical[\"cortical_thickness\"],\n            \"predicted_epiphyseal_plate_width\": pred_clinical[\"epiphyseal_plate_width\"],\n            \"predicted_gp_bone_age_months\": pred_gp_bone_age,\n            \"psnr_pred_vs_target\": psnr, \"ssim_pred_vs_target\": ssim_val,\n            \"msssim_pred_vs_target\": msssim_val, \"lpips_pred_vs_target\": lpips_val,\n            \"output_path\": fp,\n        })\n    \n    # Create DataFrame from records\n    df = pd.DataFrame.from_records(records)\n    \n    # Define column order\n    cols = [\"id\", \"boneage\", \"sex\",\n            \"pediatric_area\", \"pediatric_height\", \"pediatric_width\", \"pediatric_aspect\", \"pediatric_density\",\n            \"pediatric_width_length_ratio\", \"pediatric_cortical_thickness\", \"pediatric_epiphyseal_plate_width\",\n            \"predicted_area\", \"predicted_height\", \"predicted_width\", \"predicted_aspect\", \"predicted_density\",\n            \"predicted_width_length_ratio\", \"predicted_cortical_thickness\", \"predicted_epiphyseal_plate_width\",\n            \"predicted_gp_bone_age_months\",\n            \"psnr_pred_vs_target\", \"ssim_pred_vs_target\", \"msssim_pred_vs_target\", \"lpips_pred_vs_target\",\n            \"output_path\"]\n    \n    # Reorder columns\n    df = df[cols]\n    \n    # Save to Excel\n    df.to_excel(excel_path, index=False)\n    print(f\"Saved {len(paths)} comparison images to {out_dir}\")\n    print(f\"Excel: {excel_path}\")\n\n    # Visualize metrics\n    metrics = [\"area\", \"height\", \"width\", \"density\", \"cortical_thickness\", \"epiphyseal_plate_width\"]\n    plt.figure(figsize=(15, 10))\n    \n    # Create subplots for each metric\n    for i, metric in enumerate(metrics, 1):\n        plt.subplot(2, 3, i)\n        plt.hist(df[f\"pediatric_{metric}\"], bins=30, alpha=0.5, label=\"Pediatric\", color=\"blue\")\n        plt.hist(df[f\"predicted_{metric}\"], bins=30, alpha=0.5, label=\"Predicted (216m)\", color=\"orange\")\n        plt.title(f\"{metric.capitalize()} Distribution\")\n        plt.xlabel(metric.capitalize())\n        plt.ylabel(\"Count\")\n        plt.legend()\n    \n    # Adjust layout and save\n    plt.tight_layout()\n    plot_path = os.path.join(out_dir, \"metrics_histogram.png\")\n    plt.savefig(plot_path, dpi=300)\n    plt.close()\n    print(f\"Metrics visualization saved at: {plot_path}\")\n\n    return paths, excel_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.802061Z","iopub.execute_input":"2025-10-02T16:19:24.802315Z","iopub.status.idle":"2025-10-02T16:19:24.822384Z","shell.execute_reply.started":"2025-10-02T16:19:24.802292Z","shell.execute_reply":"2025-10-02T16:19:24.821810Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"**Inference and Export Function**","metadata":{}},{"cell_type":"code","source":"# Function to run inference and export results\n@torch.no_grad()\ndef infer_val_and_export(\n    net_ckpt: str,\n    ds: BoneAgeCondDiffusionDS,\n    val_indices: List[int],\n    out_dir: str = CFG.OUT_DIR,\n    excel_path: str = CFG.MEAS_XLSX\n):\n    # Create output directory\n    os.makedirs(out_dir, exist_ok=True)\n    \n    # Initialize model\n    net = CondUNet(cvec_dim=14).to(DEVICE)\n    \n    # Load checkpoint\n    sd = torch.load(net_ckpt, map_location=DEVICE)\n    raw = sd[\"model\"] if isinstance(sd, dict) and \"model\" in sd else sd\n    raw = clean_state_dict_prefixes(raw)\n    net.load_state_dict(raw, strict=False)\n    \n    # Initialize LPIPS model\n    lpips_alex = None\n    if lpips is not None:\n        try:\n            lpips_alex = lpips.LPIPS(net='alex').to(DEVICE).eval()\n        except Exception:\n            lpips_alex = None\n    \n    # Try to load font\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 16)\n    except Exception:\n        font = ImageFont.load_default()\n    \n    # Initialize records and paths\n    records, paths = [], []\n    \n    # Iterate over validation indices\n    for i in tqdm(val_indices, desc=\"Infer+Export (val)\"):\n        # Get row data\n        row = ds.df.iloc[i]\n        path = row[\"img_path\"]\n        sex = int(row[\"male\"])\n        boneage = int(row.get(\"boneage\", -1))\n        \n        # Extract image ID\n        img_id = int(_ID_RE.search(os.path.basename(path)).group(1)) if _ID_RE.search(os.path.basename(path)) else i\n        sex_str = \"Male\" if sex == 1 else \"Female\"\n        \n        # Load and process image\n        base_im = load_gray_png_any(path, CFG.IMG_SIZE)\n        base_u8 = np.array(base_im, dtype=np.uint8)\n        \n        # Get measurements and mask\n        pedi_meas, mask_full = robust_measurements_and_mask(base_u8)\n        \n        # Convert to tensors\n        pedi = pil_to_tensor_gray(base_im).to(DEVICE)\n        mask = torch.from_numpy((mask_full.astype(np.float32)/255.0))[None,None,:,:].to(DEVICE) * 2.0 - 1.0\n        \n        # Build parameter vector\n        cvec = build_param_vec(CFG.TARGET_AGE, sex, pedi_meas).unsqueeze(0).to(DEVICE)\n        \n        # Generate prediction\n        pred = ddim_sample(net, pedi, mask, cvec, steps=CFG.DDIM_STEPS, eta=0.0, cfg_w=CFG.CFG_WEIGHT)\n        pred_im = tensor_to_pil_gray(pred)\n        pred_u8 = np.array(pred_im, dtype=np.uint8)\n        \n        # Get measurements for prediction\n        pred_meas, pred_mask = robust_measurements_and_mask(pred_u8)\n        \n        # Generate target image\n        tgt_im = morph_adultize_at_age(base_im, sex, CFG.TARGET_AGE)\n        tgt_u8 = np.array(tgt_im, dtype=np.uint8)\n        \n        # Compute metrics\n        psnr = compute_psnr(pred_u8, tgt_u8)\n        ssim_val = compute_ssim(pred_u8, tgt_u8)\n        msssim_val = compute_msssim(pred_u8, tgt_u8)\n        lpips_val = compute_lpips(pred_u8, tgt_u8, lpips_alex)\n        \n        # Compute clinical metrics\n        pedi_clinical = compute_clinical_metrics(base_u8, mask_full)\n        pred_clinical = compute_clinical_metrics(pred_u8, pred_mask)\n        \n        # Estimate GP bone age\n        pred_gp_bone_age = estimate_gp_bone_age(pred_u8, pred_mask, sex)\n        \n        # Create composite image\n        composite = Image.new('L', (2 * CFG.IMG_SIZE, CFG.IMG_SIZE + 30))\n        composite.paste(base_im, (0, 30))\n        composite.paste(pred_im, (CFG.IMG_SIZE, 30))\n        \n        # Add text annotations\n        draw = ImageDraw.Draw(composite)\n        draw.text((10, 5), f\"Pediatric: ID={img_id}, Age={boneage}m, {sex_str}\", fill=255, font=font)\n        draw.text((CFG.IMG_SIZE + 10, 5), f\"Predicted: 216m (GP est: {pred_gp_bone_age}m)\", fill=255, font=font)\n        \n        # Save composite image\n        fp = os.path.join(out_dir, f\"{img_id}_{boneage}m_{sex_str}_vs_predicted_216m.png\")\n        composite.save(fp)\n        paths.append(fp)\n        \n        # Add record\n        records.append({\n            \"id\": img_id, \"boneage\": boneage, \"sex\": sex_str,\n            \"pediatric_area\": pedi_meas[\"area\"], \"pediatric_height\": pedi_meas[\"height\"],\n            \"pediatric_width\": pedi_meas[\"width\"], \"pediatric_aspect\": pedi_meas[\"aspect\"],\n            \"pediatric_density\": pedi_meas[\"density\"], \"pediatric_width_length_ratio\": pedi_meas[\"width_length_ratio\"],\n            \"pediatric_cortical_thickness\": pedi_clinical[\"cortical_thickness\"],\n            \"pediatric_epiphyseal_plate_width\": pedi_clinical[\"epiphyseal_plate_width\"],\n            \"predicted_area\": pred_meas[\"area\"], \"predicted_height\": pred_meas[\"height\"],\n            \"predicted_width\": pred_meas[\"width\"], \"predicted_aspect\": pred_meas[\"aspect\"],\n            \"predicted_density\": pred_meas[\"density\"], \"predicted_width_length_ratio\": pred_meas[\"width_length_ratio\"],\n            \"predicted_cortical_thickness\": pred_clinical[\"cortical_thickness\"],\n            \"predicted_epiphyseal_plate_width\": pred_clinical[\"epiphyseal_plate_width\"],\n            \"predicted_gp_bone_age_months\": pred_gp_bone_age,\n            \"psnr_pred_vs_target\": psnr, \"ssim_pred_vs_target\": ssim_val,\n            \"msssim_pred_vs_target\": msssim_val, \"lpips_pred_vs_target\": lpips_val,\n            \"output_path\": fp,\n        })\n    \n    # Create DataFrame from records\n    df = pd.DataFrame.from_records(records)\n    \n    # Define column order\n    cols = [\"id\", \"boneage\", \"sex\",\n            \"pediatric_area\", \"pediatric_height\", \"pediatric_width\", \"pediatric_aspect\", \"pediatric_density\",\n            \"pediatric_width_length_ratio\", \"pediatric_cortical_thickness\", \"pediatric_epiphyseal_plate_width\",\n            \"predicted_area\", \"predicted_height\", \"predicted_width\", \"predicted_aspect\", \"predicted_density\",\n            \"predicted_width_length_ratio\", \"predicted_cortical_thickness\", \"predicted_epiphyseal_plate_width\",\n            \"predicted_gp_bone_age_months\",\n            \"psnr_pred_vs_target\", \"ssim_pred_vs_target\", \"msssim_pred_vs_target\", \"lpips_pred_vs_target\",\n            \"output_path\"]\n    \n    # Reorder columns\n    df = df[cols]\n    \n    # Save to Excel\n    df.to_excel(excel_path, index=False)\n    print(f\"Saved {len(paths)} comparison images to {out_dir}\")\n    print(f\"Excel: {excel_path}\")\n\n    # Visualize metrics\n    metrics = [\"area\", \"height\", \"width\", \"density\", \"cortical_thickness\", \"epiphyseal_plate_width\"]\n    plt.figure(figsize=(15, 10))\n    \n    # Create subplots for each metric\n    for i, metric in enumerate(metrics, 1):\n        plt.subplot(2, 3, i)\n        plt.hist(df[f\"pediatric_{metric}\"], bins=30, alpha=0.5, label=\"Pediatric\", color=\"blue\")\n        plt.hist(df[f\"predicted_{metric}\"], bins=30, alpha=0.5, label=\"Predicted (216m)\", color=\"orange\")\n        plt.title(f\"{metric.capitalize()} Distribution\")\n        plt.xlabel(metric.capitalize())\n        plt.ylabel(\"Count\")\n        plt.legend()\n    \n    # Adjust layout and save\n    plt.tight_layout()\n    plot_path = os.path.join(out_dir, \"metrics_histogram.png\")\n    plt.savefig(plot_path, dpi=300)\n    plt.close()\n    print(f\"Metrics visualization saved at: {plot_path}\")\n\n    return paths, excel_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.823164Z","iopub.execute_input":"2025-10-02T16:19:24.823843Z","iopub.status.idle":"2025-10-02T16:19:24.840572Z","shell.execute_reply.started":"2025-10-02T16:19:24.823821Z","shell.execute_reply":"2025-10-02T16:19:24.840003Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"*******Data Loaders*******","metadata":{}},{"cell_type":"code","source":"# Function to create data loaders\ndef make_loaders() -> Tuple[DataLoader, DataLoader, 'BoneAgeCondDiffusionDS', List[int]]:\n    # Find dataset\n    csv_path, img_dir = find_rsna_boneage_root()\n    \n    # Create dataset\n    ds = BoneAgeCondDiffusionDS(csv_path, img_dir, img_size=CFG.IMG_SIZE, max_samples=CFG.MAX_SAMPLES)\n    n = len(ds)\n    \n    # Handle empty dataset\n    if n == 0: \n        raise RuntimeError(\"Empty dataset.\")\n    \n    # Handle single sample case\n    if n == 1:\n        tr = Subset(ds, [0])\n        va = Subset(ds, [0])\n        val_indices = [0]\n        train_loader = DataLoader(tr, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\n        val_loader   = DataLoader(va, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n        return train_loader, val_loader, ds, val_indices\n    \n    # Split dataset\n    n_train = max(1, int(0.9 * n))\n    n_val = max(1, n - n_train)\n    if n_train == n: \n        n_train, n_val = n - 1, 1\n    \n    # Create random split\n    gen = torch.Generator().manual_seed(CFG.SEED)\n    tr_subset, va_subset = random_split(ds, [n_train, n_val], generator=gen)\n    val_indices = va_subset.indices\n    \n    # Create data loaders\n    train_loader = DataLoader(tr_subset, batch_size=CFG.BATCH, shuffle=True,\n                              num_workers=CFG.NUM_WORKERS, pin_memory=True,\n                              persistent_workers=(CFG.NUM_WORKERS > 0))\n    val_loader   = DataLoader(va_subset, batch_size=CFG.BATCH, shuffle=False,\n                              num_workers=CFG.NUM_WORKERS, pin_memory=True,\n                              persistent_workers=(CFG.NUM_WORKERS > 0))\n    \n    return train_loader, val_loader, ds, val_indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.841171Z","iopub.execute_input":"2025-10-02T16:19:24.841389Z","iopub.status.idle":"2025-10-02T16:19:24.852315Z","shell.execute_reply.started":"2025-10-02T16:19:24.841374Z","shell.execute_reply":"2025-10-02T16:19:24.851654Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"****Main Execution****","metadata":{}},{"cell_type":"code","source":"# Main execution block\nif __name__ == \"__main__\":\n    # Create data loaders\n    print(\"Building loaders...\")\n    train_loader, val_loader, ds, val_indices = make_loaders()\n    print(f\"Dataset size: {len(ds)}\")\n    print(ds.df.head()[[\"id\", \"male\", \"boneage\", \"img_path\"]])\n\n    # Set dataset to training mode\n    ds.train()\n    \n    # Initialize model\n    net = CondUNet(cvec_dim=14).to(DEVICE)\n    net = load_pretrained_unet(net)\n    \n    # Define checkpoint path\n    ckpt_path = os.path.join(CFG.SAVE_DIR, \"cond_diffusion_mask_prior.pth\")\n\n    # Train model\n    print(\"Training...\")\n    ckpt_path = train_diffusion_amp_accum(net, train_loader, val_loader, epochs=CFG.EPOCHS, save_path=ckpt_path)\n\n    # Generate predictions and export\n    print(\"Generating 216-month predictions + side-by-side images + Excel...\")\n    ds.eval()\n    os.makedirs(CFG.OUT_DIR, exist_ok=True)\n    paths, xlsx_path = infer_val_and_export(ckpt_path, ds, val_indices, out_dir=CFG.OUT_DIR, excel_path=CFG.MEAS_XLSX)\n\n    # Save file list\n    with open(os.path.join(CFG.OUT_DIR, \"files.json\"), \"w\") as f:\n        json.dump(sorted(paths), f, indent=2)\n    \n    print(\"Done.\")\n    print(f\"Excel saved at: {xlsx_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T16:19:24.853009Z","iopub.execute_input":"2025-10-02T16:19:24.853251Z","iopub.status.idle":"2025-10-02T19:07:44.662819Z","shell.execute_reply.started":"2025-10-02T16:19:24.853235Z","shell.execute_reply":"2025-10-02T19:07:44.662063Z"}},"outputs":[{"name":"stdout","text":"Building loaders...\n[data] CSV: /kaggle/input/rsna-bone-age/boneage-training-dataset.csv\n[data] IMG_DIR: /kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset (files: 12611)\n[data] Indexed 12611 images.\nDataset size: 12611\n     id  male  boneage                                           img_path\n0  1377     0      180  /kaggle/input/rsna-bone-age/boneage-training-d...\n1  1378     0       12  /kaggle/input/rsna-bone-age/boneage-training-d...\n2  1379     0       94  /kaggle/input/rsna-bone-age/boneage-training-d...\n3  1380     1      120  /kaggle/input/rsna-bone-age/boneage-training-d...\n4  1381     0       82  /kaggle/input/rsna-bone-age/boneage-training-d...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b4669ff21a4dd8949aa839a22e2bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/143M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05840e0382e4af490c194bb64ae7105"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/1139538167.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=CFG.AMP and DEVICE.type == \"cuda\")\n","output_type":"stream"},{"name":"stdout","text":"Loaded pretrained weights from google/ddpm-cifar10-32\nTraining...\nAMP=True, ACCUM_STEPS=2, eff_batch=16\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 1/10: 100%|| 1419/1419 [11:56<00:00,  1.98it/s, loss=0.1439, lr=0.000100]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 1] train=0.1439 val=0.1115 lr=0.000098\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.1115)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 2/10: 100%|| 1419/1419 [12:00<00:00,  1.97it/s, loss=0.1022, lr=0.000098]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 2] train=0.1022 val=0.0927 lr=0.000091\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0927)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 3/10: 100%|| 1419/1419 [11:58<00:00,  1.97it/s, loss=0.0905, lr=0.000091]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 3] train=0.0905 val=0.0845 lr=0.000080\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0845)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 4/10: 100%|| 1419/1419 [11:58<00:00,  1.97it/s, loss=0.0856, lr=0.000080]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 4] train=0.0856 val=0.0798 lr=0.000066\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0798)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 5/10: 100%|| 1419/1419 [11:57<00:00,  1.98it/s, loss=0.0819, lr=0.000066]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 5] train=0.0819 val=0.0794 lr=0.000051\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0794)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 6/10: 100%|| 1419/1419 [11:57<00:00,  1.98it/s, loss=0.0801, lr=0.000051]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 6] train=0.0801 val=0.0770 lr=0.000035\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0770)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 7/10: 100%|| 1419/1419 [11:57<00:00,  1.98it/s, loss=0.0781, lr=0.000035]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 7] train=0.0781 val=0.0757 lr=0.000021\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0757)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 8/10: 100%|| 1419/1419 [11:57<00:00,  1.98it/s, loss=0.0769, lr=0.000021]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 8] train=0.0769 val=0.0756 lr=0.000010\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0756)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 9/10: 100%|| 1419/1419 [11:57<00:00,  1.98it/s, loss=0.0750, lr=0.000010]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 9] train=0.0750 val=0.0676 lr=0.000003\nSaved /kaggle/working/cond_diffusion_mask_prior.pth (best 0.0676)\n","output_type":"stream"},{"name":"stderr","text":"Train Ep 10/10: 100%|| 1419/1419 [11:58<00:00,  1.98it/s, loss=0.0755, lr=0.000003]\n","output_type":"stream"},{"name":"stdout","text":"[Epoch 10] train=0.0755 val=0.0683 lr=0.000001\nGenerating 216-month predictions + side-by-side images + Excel...\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|| 233M/233M [00:01<00:00, 210MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n","output_type":"stream"},{"name":"stderr","text":"Infer+Export (val): 100%|| 1262/1262 [40:55<00:00,  1.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Saved 1262 comparison images to /kaggle/working/out_216\nExcel: /kaggle/working/measurements_export.xlsx\nMetrics visualization saved at: /kaggle/working/out_216/metrics_histogram.png\nDone.\nExcel saved at: /kaggle/working/measurements_export.xlsx\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================\n# RSNA Bone Age  Quick Plots (Kaggle-ready)\n# Outputs -> /kaggle/working/rsna_plots\n# ============================================================\n\nimport os, glob, re, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps, ImageDraw, ImageFont\n\n# ------------- config -------------\nSEED = 42                          # why: reproducible sampling\nIMG_SIZE = 256                     # why: consistent loading for intensity calc\nSTATS_SAMPLE = 400                 # why: limit IO for size/intensity stats\nMONTAGE_N = 16                     # why: compact overview (44)\nOUT_DIR = \"/kaggle/working/rsna_plots\"\nDATASET_HINTS = [\n    \"/kaggle/input/rsna-bone-age\",\n    \"/kaggle/input/rsna-bone-age/rsna-bone-age\",\n    \"/kaggle/input\"\n]\n\nrandom.seed(SEED); np.random.seed(SEED)\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# ------------- data discovery -------------\ndef find_rsna_paths():\n    \"\"\"Find CSV + image root robustly across Kaggle mount patterns.\"\"\"\n    cands = []\n    for base in DATASET_HINTS:\n        if not os.path.exists(base):\n            continue\n        for csv_path in glob.glob(os.path.join(base, \"**\", \"boneage-training-dataset.csv\"), recursive=True):\n            root = os.path.dirname(csv_path)\n            # choose dir with most images\n            dirs = [root] + [d for d in glob.glob(os.path.join(root, \"**\"), recursive=True) if os.path.isdir(d)]\n            best, cnt = \"\", -1\n            for d in dirs:\n                n = (len(glob.glob(os.path.join(d, \"*.png\"))) +\n                     len(glob.glob(os.path.join(d, \"*.jpg\"))) +\n                     len(glob.glob(os.path.join(d, \"*.jpeg\"))))\n                if n > cnt:\n                    best, cnt = d, n\n            cands.append((csv_path, best, cnt))\n    if not cands:\n        raise FileNotFoundError(\"Attach Kaggle dataset: kmader/rsna-bone-age (Add data  search).\")\n    csv_path, img_dir, cnt = sorted(cands, key=lambda x: -x[2])[0]\n    print(f\"[data] CSV: {csv_path}\\n[data] IMG_DIR: {img_dir} (files: {cnt})\")\n    return csv_path, img_dir\n\n_ID_RE = re.compile(r\"(\\d+)\\.(png|jpg|jpeg)$\", re.IGNORECASE)\n\ndef index_image_dir(img_dir: str):\n    \"\"\"Map image id  canonical path (prefer shallower path).\"\"\"\n    idx = {}\n    for p in glob.glob(os.path.join(img_dir, \"**\", \"*.*\"), recursive=True):\n        if not os.path.isfile(p): \n            continue\n        m = _ID_RE.search(os.path.basename(p))\n        if not m:\n            continue\n        i = int(m.group(1))\n        if i not in idx or p.count(os.sep) < idx[i].count(os.sep):\n            idx[i] = p\n    print(f\"[data] Indexed {len(idx)} images\")\n    return idx\n\n# ------------- io helpers -------------\ndef load_gray_any(path: str, size: int = None) -> Image.Image:\n    \"\"\"Open image as grayscale; optional resize for standardized stats.\"\"\"\n    im = Image.open(path).convert(\"L\")\n    if size is not None and im.size != (size, size):\n        im = im.resize((size, size), Image.LANCZOS)\n    return im\n\ndef save_fig(path: str):\n    \"\"\"Consistent figure export.\"\"\"\n    plt.tight_layout()\n    plt.savefig(path, dpi=220, bbox_inches=\"tight\")\n    plt.close()\n\n# ------------- plots -------------\ndef plot_age_hist(df: pd.DataFrame, path: str):\n    \"\"\"Bone age distribution (months).\"\"\"\n    ages = df[\"boneage\"].astype(float).values\n    plt.figure(figsize=(7,4))\n    plt.hist(ages, bins=40)\n    plt.title(\"Bone Age Distribution (months)\")\n    plt.xlabel(\"Bone Age (months)\"); plt.ylabel(\"Count\")\n    save_fig(path)\n\ndef plot_sex_bar(df: pd.DataFrame, path: str):\n    \"\"\"Sex counts.\"\"\"\n    counts = df[\"male\"].astype(int).value_counts().sort_index()\n    labels = [\"Female\",\"Male\"]\n    vals = [counts.get(0,0), counts.get(1,0)]\n    plt.figure(figsize=(5,4))\n    plt.bar(labels, vals)\n    plt.title(\"Sex Distribution\"); plt.ylabel(\"Count\")\n    save_fig(path)\n\ndef plot_age_by_sex_box(df: pd.DataFrame, path: str):\n    \"\"\"Age-by-sex boxplot.\"\"\"\n    m = df[df[\"male\"]==1][\"boneage\"].astype(float).values\n    f = df[df[\"male\"]==0][\"boneage\"].astype(float).values\n    plt.figure(figsize=(6,4))\n    plt.boxplot([f, m], labels=[\"Female\",\"Male\"], showfliers=False)\n    plt.title(\"Bone Age by Sex\"); plt.ylabel(\"Bone Age (months)\")\n    save_fig(path)\n\ndef plot_age_hist_by_sex(df: pd.DataFrame, path: str):\n    \"\"\"Overlaid age histograms by sex (quick check of balance).\"\"\"\n    m = df[df[\"male\"]==1][\"boneage\"].astype(float).values\n    f = df[df[\"male\"]==0][\"boneage\"].astype(float).values\n    plt.figure(figsize=(7,4))\n    plt.hist(f, bins=40, alpha=0.6, label=\"Female\")\n    plt.hist(m, bins=40, alpha=0.6, label=\"Male\")\n    plt.title(\"Bone Age Histogram by Sex\"); plt.xlabel(\"Bone Age (months)\")\n    plt.ylabel(\"Count\"); plt.legend()\n    save_fig(path)\n\ndef plot_image_size_hist(paths: list[str], path: str):\n    \"\"\"Distribution of raw image sizes (sample).\"\"\"\n    wh = []\n    for p in paths:\n        try:\n            with Image.open(p) as im:\n                wh.append(im.size)\n        except Exception:\n            pass\n    if not wh:\n        return\n    w = np.array([t[0] for t in wh]); h = np.array([t[1] for t in wh])\n    plt.figure(figsize=(7,4))\n    plt.hist2d(w, h, bins=30)\n    plt.colorbar(label=\"Count\")\n    plt.title(\"Raw Image Size Distribution\"); plt.xlabel(\"Width\"); plt.ylabel(\"Height\")\n    save_fig(path)\n\ndef plot_mean_intensity_hist(paths: list[str], path: str):\n    \"\"\"Mean intensity distribution (after resize to IMG_SIZE).\"\"\"\n    vals = []\n    for p in paths:\n        try:\n            im = load_gray_any(p, IMG_SIZE)\n            vals.append(np.array(im, dtype=np.uint8).mean())\n        except Exception:\n            pass\n    if not vals:\n        return\n    plt.figure(figsize=(6,4))\n    plt.hist(np.array(vals), bins=40)\n    plt.title(\"Mean Image Intensity (grayscale)\"); plt.xlabel(\"Mean Intensity (0255)\")\n    plt.ylabel(\"Count\")\n    save_fig(path)\n\ndef make_montage(df: pd.DataFrame, idx_map: dict[int,str], n: int, out_path: str):\n    \"\"\"44 montage with ID/age/sex; fast sanity-check gallery.\"\"\"\n    n = min(n, len(df))\n    samp = df.sample(n, random_state=SEED).reset_index(drop=True)\n    grid = int(np.ceil(np.sqrt(n)))\n    cell = IMG_SIZE // 2  # smaller tiles to fit on page\n    pad = 8\n\n    # pick a readable font; fallback to default\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 14)\n    except Exception:\n        font = ImageFont.load_default()\n\n    W = grid*cell + (grid+1)*pad\n    H = grid*cell + (grid+1)*pad + 30\n    canvas = Image.new(\"RGB\", (W, H), (255,255,255))\n    draw = ImageDraw.Draw(canvas)\n    draw.text((pad, pad//2), \"RSNA Sample Montage (ID | Age m | Sex)\", fill=(30,41,59), font=font)\n\n    for k, row in samp.iterrows():\n        img_id = int(row[\"id\"]); age = int(row.get(\"boneage\",-1)); sex = \"M\" if int(row[\"male\"])==1 else \"F\"\n        path = idx_map.get(img_id, None); \n        if path is None: \n            continue\n        try:\n            im = load_gray_any(path)\n            im = ImageOps.equalize(im)   # why: mild contrast normalization\n            im = im.resize((cell, cell), Image.LANCZOS)\n            # convert to RGB for labels\n            im_rgb = Image.merge(\"RGB\", (im, im, im))\n            lbl = f\"{img_id} | {age} | {sex}\"\n            ImageDraw.Draw(im_rgb).rectangle([0,0,im_rgb.width,20], fill=(255,255,255))\n            ImageDraw.Draw(im_rgb).text((4,2), lbl, fill=(30,41,59), font=font)\n            r, c = divmod(k, grid)\n            x = pad + c*(cell+pad); y = pad + 24 + r*(cell+pad)\n            canvas.paste(im_rgb, (x, y))\n        except Exception:\n            continue\n\n    canvas.save(out_path)\n\n# ------------- main -------------\ndef main():\n    csv_path, img_dir = find_rsna_paths()\n    df = pd.read_csv(csv_path)\n    df[\"male\"] = df[\"male\"].astype(int)\n    idx_map = index_image_dir(img_dir)\n    df = df[df[\"id\"].astype(int).isin(idx_map.keys())].reset_index(drop=True)\n    if len(df) == 0:\n        raise RuntimeError(\"No images matched IDs in CSV.\")\n\n    # Basic plots\n    plot_age_hist(df, os.path.join(OUT_DIR, \"age_hist.png\"))\n    plot_sex_bar(df, os.path.join(OUT_DIR, \"sex_bar.png\"))\n    plot_age_by_sex_box(df, os.path.join(OUT_DIR, \"age_by_sex_box.png\"))\n    plot_age_hist_by_sex(df, os.path.join(OUT_DIR, \"age_hist_by_sex.png\"))\n\n    # Sample for IO-heavy stats\n    ids = df[\"id\"].astype(int).tolist()\n    if len(ids) > STATS_SAMPLE:\n        ids = random.sample(ids, STATS_SAMPLE)\n    paths = [idx_map[i] for i in ids if i in idx_map]\n\n    plot_image_size_hist(paths, os.path.join(OUT_DIR, \"image_size_hist2d.png\"))\n    plot_mean_intensity_hist(paths, os.path.join(OUT_DIR, \"mean_intensity_hist.png\"))\n    make_montage(df, idx_map, MONTAGE_N, os.path.join(OUT_DIR, \"montage_4x4.png\"))\n\n    print(\"Saved plots:\")\n    for p in sorted(glob.glob(os.path.join(OUT_DIR, \"*.png\"))):\n        print(\" -\", p)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T19:52:39.459096Z","iopub.execute_input":"2025-10-02T19:52:39.459350Z","iopub.status.idle":"2025-10-02T19:54:08.739458Z","shell.execute_reply.started":"2025-10-02T19:52:39.459330Z","shell.execute_reply":"2025-10-02T19:54:08.738863Z"}},"outputs":[{"name":"stdout","text":"[data] CSV: /kaggle/input/rsna-bone-age/boneage-training-dataset.csv\n[data] IMG_DIR: /kaggle/input/rsna-bone-age/boneage-training-dataset/boneage-training-dataset (files: 12611)\n[data] Indexed 12611 images\nSaved plots:\n - /kaggle/working/rsna_plots/age_by_sex_box.png\n - /kaggle/working/rsna_plots/age_hist.png\n - /kaggle/working/rsna_plots/age_hist_by_sex.png\n - /kaggle/working/rsna_plots/image_size_hist2d.png\n - /kaggle/working/rsna_plots/mean_intensity_hist.png\n - /kaggle/working/rsna_plots/montage_4x4.png\n - /kaggle/working/rsna_plots/sex_bar.png\n","output_type":"stream"}],"execution_count":1}]}